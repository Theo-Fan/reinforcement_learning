{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f57f56",
   "metadata": {},
   "source": [
    "# Flax basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e71fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py39/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: flax in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.8.5)\n",
      "Requirement already satisfied: jax in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.4.30)\n",
      "Requirement already satisfied: optax in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.2.4)\n",
      "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (1.26.4)\n",
      "Requirement already satisfied: msgpack in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (1.1.0)\n",
      "Requirement already satisfied: orbax-checkpoint in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (0.6.4)\n",
      "Requirement already satisfied: tensorstore in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (0.1.69)\n",
      "Requirement already satisfied: rich>=11.1 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (13.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from flax) (6.0.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from jax) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from jax) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from jax) (8.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from optax) (2.1.0)\n",
      "Requirement already satisfied: chex>=0.1.87 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from optax) (0.1.89)\n",
      "Requirement already satisfied: etils[epy] in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from optax) (1.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from chex>=0.1.87->optax) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from importlib-metadata>=4.6->jax) (3.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from rich>=11.1->flax) (2.18.0)\n",
      "Requirement already satisfied: nest_asyncio in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from orbax-checkpoint->flax) (4.25.4)\n",
      "Requirement already satisfied: humanize in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from orbax-checkpoint->flax) (4.12.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from etils[epy]->optax) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/anaconda3/envs/py39/lib/python3.9/site-packages (from etils[epy]->optax) (6.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install flax jax jaxlib optax gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0fef0",
   "metadata": {},
   "source": [
    "## DQN in CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cabb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from typing import Sequence\n",
    "from flax.training.train_state import TrainState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd0a9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network using flax.linen\n",
    "\n",
    "class q_network(nn.Module):\n",
    "    hidden_dims: Sequence[int]\n",
    "    n_actions: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\" Network architecture:\n",
    "            input layer(state_dim)\n",
    "            ==> hidden layer(128) + relu \n",
    "            ==> hidden layer(128) + relu\n",
    "            ==> output layer(n_actions)\n",
    "        \"\"\"\n",
    "        for h in self.hidden_dims:\n",
    "            x = nn.Dense(h)(x)\n",
    "            x = nn.relu(x)\n",
    "        return nn.Dense(self.n_actions)(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7700cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, ne_state, done):\n",
    "        self.buffer.append((state, action, reward, ne_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)    \n",
    "        states, actions, rewards, ne_states, dones = map(np.array, zip(*samples))\n",
    "        return states, actions, rewards, ne_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391d7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN utils\n",
    "\n",
    "@jax.jit\n",
    "def select_action(params, state, epsilon, rng, n_actions):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)  # Explore\n",
    "    q_values = q_net(params, state)\n",
    "    return int(jnp.argmax(q_values))  # Exploit\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: TrainState, batch, gamma: float):\n",
    "    def loss_fn(params):\n",
    "        states, actions, rewards, ne_states, dones = batch\n",
    "        q_values = q_net.apply(params, states)\n",
    "        q_actions = jnp.take_along_axis(q_values, actions[..., None], axis=1).squeeze()  # get value\n",
    "        \n",
    "        next_q_values = q_net.apply(state.params, ne_states)\n",
    "        max_next_q = jnp.max(next_q_values, axis=1)\n",
    "        \n",
    "        target = rewards + (1 - dones) * gamma * max_next_q\n",
    "        \n",
    "        loss = jnp.mean((q_actions - target) ** 2)  # MSE loss\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f74a9296",
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function select_action at /var/folders/gy/6wf0tc3n7276h9vd9dqcpkvw0000gn/T/ipykernel_48762/608396390.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument epsilon.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m     33\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 34\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     38\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(params, state, epsilon, rng, n_actions)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(params, state, epsilon, rng, n_actions):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n_actions \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Explore\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m q_net(params, state)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/jax/_src/core.py:1517\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1517\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function select_action at /var/folders/gy/6wf0tc3n7276h9vd9dqcpkvw0000gn/T/ipykernel_48762/608396390.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument epsilon.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "q_net = q_network(hidden_dims=[128, 128], n_actions=n_actions)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# init model and optimizer\n",
    "lr = 1e-3\n",
    "input_dim = jnp.ones((state_dim,))\n",
    "params = q_net.init(rng, input_dim)\n",
    "tx = optax.adam(learning_rate=lr)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=q_net.apply,\n",
    "    params=params,\n",
    "    tx=tx\n",
    ")\n",
    "\n",
    "# hyperparameters\n",
    "buffer = ReplayBuffer()\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.05\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(200):\n",
    "        state_tensor = jnp.array(state, dtype=jnp.float32)\n",
    "        actions = select_action(train_state.params, state_tensor, epsilon, rng, n_actions)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(actions)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(buffer) > batch_size:\n",
    "            states, actions, rewards, ne_states, dones = buffer.sample(batch_size)\n",
    "            batch = (\n",
    "                jnp.array(states, dtype=jnp.float32),\n",
    "                jnp.array(actions, dtype=jnp.int32),\n",
    "                jnp.array(rewards, dtype=jnp.float32),\n",
    "                jnp.array(ne_states),\n",
    "                jnp.array(dones, dtype=jnp.bool_)\n",
    "            )\n",
    "            \n",
    "            train_state, loss = train_step(train_state, batch, gamma)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
