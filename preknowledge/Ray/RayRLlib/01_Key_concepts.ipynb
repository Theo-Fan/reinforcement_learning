{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-15T05:09:39.978003Z",
     "start_time": "2025-07-15T05:09:28.800261Z"
    }
   },
   "source": [
    "import tree  # pip install dm_tree\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.single_agent_env_runner import SingleAgentEnvRunner\n",
    "\n",
    "# Configure the EnvRunners.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Acrobot-v1\")\n",
    "    .env_runners(num_env_runners=2, num_envs_per_env_runner=1)\n",
    ")\n",
    "# Create the EnvRunner actors.\n",
    "env_runners = [\n",
    "    ray.remote(SingleAgentEnvRunner).remote(config=config)\n",
    "    for _ in range(config.num_env_runners)\n",
    "]\n",
    "\n",
    "# Gather lists of `SingleAgentEpisode`s (each EnvRunner actor returns one\n",
    "# such list with exactly two episodes in it).\n",
    "episodes = ray.get([\n",
    "    er.sample.remote(num_episodes=3)\n",
    "    for er in env_runners\n",
    "])\n",
    "# Two remote EnvRunners used.\n",
    "assert len(episodes) == 2\n",
    "# Each EnvRunner returns three episodes\n",
    "assert all(len(eps_list) == 3 for eps_list in episodes)\n",
    "\n",
    "# Report the returns of all episodes collected\n",
    "for episode in tree.flatten(episodes):\n",
    "    print(\"R=\", episode.get_return())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 13:09:34,439\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8266 \u001B[39m\u001B[22m\n",
      "\u001B[36m(SingleAgentEnvRunner pid=96298)\u001B[0m 2025-07-15 13:09:39,622\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R= -500.0\n",
      "R= -500.0\n",
      "R= -500.0\n",
      "R= -500.0\n",
      "R= -388.0\n",
      "R= -500.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T06:06:42.495950Z",
     "start_time": "2025-07-15T06:06:32.996162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Configure and build an initial algorithm.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    ")\n",
    "ppo = config.build()\n",
    "\n",
    "# Train for one iteration, then save to a checkpoint.\n",
    "print(ppo.train())\n",
    "checkpoint_dir = ppo.save_to_path()\n",
    "print(f\"saved algo to {checkpoint_dir}\")"
   ],
   "id": "41d753d146edcf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 14:06:33,004\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-07-15 14:06:33,010\tWARNING algorithm_config.py:4703 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/opt/anaconda3/envs/rl/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:512: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/anaconda3/envs/rl/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/anaconda3/envs/rl/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/anaconda3/envs/rl/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-07-15 14:06:39,462\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-07-15 14:06:40,193\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timers': {'training_iteration': 2.279638792038895, 'restore_env_runners': 1.2083910405635834e-05, 'training_step': 2.2795353330438957, 'env_runner_sampling_timer': 0.4964607080910355, 'learner_update_timer': 1.7798381249886006, 'synch_weights': 0.0030837920494377613}, 'env_runners': {'episode_return_mean': -1560.5890724657588, 'weights_seq_no': 0.0, 'timers': {'connectors': {'TensorToNumpy': 1.2900714393356271e-05, 'NormalizeAndClipActions': 1.7433756679771867e-05, 'AddTimeDimToBatchAndZeroPad': 2.09595676856077e-06, 'BatchIndividualItems': 5.527426389743975e-06, 'NumpyToTensor': 8.549708406301404e-06, 'AddObservationsFromEpisodesToBatch': 2.74782993432141e-06, 'UnBatchToIndividualItems': 5.518644705516158e-06, 'AddStatesFromEpisodesToBatch': 1.541302740607286e-06, 'ListifyDataForVectorEnv': 9.113648814768929e-06, 'GetActions': 3.490234622012163e-05, 'RemoveSingleTsTimeRankFromBatch': 4.996858600128309e-07}}, 'episode_return_max': -1351.899365154296, 'num_module_steps_sampled_lifetime': {'default_policy': 4000}, 'module_episode_returns_mean': {'default_policy': -1560.5890724657588}, 'episode_duration_sec_mean': 0.04756929994910024, 'episode_len_max': 200, 'num_agent_steps_sampled': {'default_agent': 4000}, 'sample': 0.4888226870098151, 'episode_return_min': -1825.6249256666263, 'episode_len_min': 200, 'num_module_steps_sampled': {'default_policy': 4000}, 'env_to_module_sum_episodes_length_out': 131.73765398537716, 'env_to_module_sum_episodes_length_in': 131.73765398537716, 'episode_len_mean': 200.0, 'num_env_steps_sampled_lifetime': 4000, 'agent_episode_returns_mean': {'default_agent': -1560.5890724657588}, 'num_episodes': 20, 'num_agent_steps_sampled_lifetime': {'default_agent': 4000}, 'num_episodes_lifetime': 20, 'num_env_steps_sampled': 4000, 'num_env_steps_sampled_lifetime_throughput': nan}, 'learners': {'__all_modules__': {'learner_connector_sum_episodes_length_in': 4000, 'learner_connector_sum_episodes_length_out': 4020, 'timers': {'connectors': {'AddObservationsFromEpisodesToBatch': 8.058303501456976e-05, 'AddColumnsFromEpisodesToTrainBatch': 0.025208249920979142, 'AddStatesFromEpisodesToBatch': 3.0830269679427147e-06, 'GeneralAdvantageEstimation': 0.03194400004576892, 'AddOneTsToEpisodesAndTruncate': 0.0017342909704893827, 'AddTimeDimToBatchAndZeroPad': 1.8832972273230553e-05, 'BatchIndividualItems': 0.023347334004938602, 'NumpyToTensor': 0.00047616695519536734}}, 'num_non_trainable_parameters': 0, 'num_module_steps_trained_lifetime': 4020, 'num_env_steps_trained_lifetime': 4020, 'num_trainable_parameters': 134403, 'num_module_steps_trained': 4020, 'num_env_steps_trained': 4020, 'num_env_steps_trained_lifetime_throughput': nan}, 'default_policy': {'diff_num_grad_updates_vs_sampler_policy': 0.0, 'total_loss': 10.018760681152344, 'vf_loss_unclipped': 263337.125, 'entropy': 1.207816243171692, 'module_train_batch_size_mean': 4020, 'num_module_steps_trained': 4020, 'mean_kl_loss': 0.018045516684651375, 'curr_kl_coeff': 0.20000000298023224, 'default_optimizer_learning_rate': 5e-05, 'vf_explained_var': -5.614757537841797e-05, 'curr_entropy_coeff': 0.0, 'vf_loss': 10.0, 'num_non_trainable_parameters': 0, 'gradients_default_optimizer_global_norm': 0.43678075075149536, 'num_module_steps_trained_lifetime': 4020, 'weights_seq_no': 1.0, 'num_trainable_parameters': 134403, 'policy_loss': 0.015152023173868656}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 4000, 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': nan, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-07-15_14-06-42', 'timestamp': 1752559602, 'time_this_iter_s': 2.284193754196167, 'time_total_s': 2.284193754196167, 'pid': 96273, 'hostname': 'Theo-MacBook-Pro.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'Pendulum-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x363c94400>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 2.284193754196167, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 47.65, 'ram_util_percent': 83.425}}\n",
      "saved algo to /var/folders/gy/6wf0tc3n7276h9vd9dqcpkvw0000gn/T/d6b3a581-3c2f-47af-8424-bb48b009b32e\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4017fb7d0dcd6a81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
